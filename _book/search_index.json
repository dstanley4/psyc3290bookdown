[
["making-your-data-ready-for-analysis.html", "Chapter 3 Making your data ready for analysis 3.1 Required Packages 3.2 Objective 3.3 Context 3.4 Begin with the end in mind 3.5 Data collection considerations 3.6 Following the examples 3.7 Entering data into spreadsheets 3.8 Experiment: Between", " Chapter 3 Making your data ready for analysis 3.1 Required Packages The following packages must be installed before starting this chapter. Required Packages apaTables HMisc janitor psych skimr tidyverse Important Note: that you should NOT use library(psych) at any point. There are major conflicts between the psych package and the tidyverse. We will access the psych package commands by preceding each command with psych:: instead of using library(psych). The following data files are used in this chapter: Required Data data_ex_between.csv data_ex_within.csv data_food.csv data_item_scoring.csv data_item_time.csv The files are available at: https://github.com/dstanley4/psyc6060bookdown 3.2 Objective In this chatper we strongly advocate for you using a naming convention for file, variable, and column names. This convention will save you hours of hassles and permit easy application of certain tidyverse command. I must stress though that although the naming convention I advocate is based on the tidyverse style guide, it is not “right” - there are other naming conventions you can use. And any naming convention is better than no naming convention. The naming convention we advocate for here will solve many problems for you. I encourage to use this system for for weeks or months over many projects - until you see the benefits of this system, and correspondingly it’s shortcomings. After you are well versed in the strengths/weaknesses of the naming conventions used here you may choose to create your own naming convention sytem. 3.3 Context Due to a number of high profile failure to replicate study results (Nosek 2015) it’s become increasingly clear that there is a general crisis of confidence in many areas of science (Baker 2016). Statistical (and other) explanations have been offered (Simmons, Nelson, and Simonsohn 2011) for why it’s hard to replicate results across different sets of data. However, scientists are also finding it challenging to recreate the numbers in their own papers using their own data. Indeed, the editor of Molecular Brain asked authors to submit the data used to create the numbers in published papers and found that the wrong data was submitted for 40 out of 41 papers (Miyakawa 2020). Consequently, some researchers have suggested that it is critical to distinguish between replication and reproducibility (Patil P. 2019). Replication refers to trying to obtain the same result from a different data sets. Reproducibility refers to trying to obtain the same results from the same data set. Unfortunately, some authors use these two terms interchangeably and fail to make any distinction between them. I encourage you to make the distinction and the use the terms consist with use suggested by (Patil P. 2019). It may seem that reproducibility should be given - but it’s not. Correspondingly, there a trend is for journals and authors to adopt Transparency and Openness Promotion (TOP) guidelines. These guidelines involve such things as making your materials, data, code, and analysis scripts available on public repositories so anyone can check your data. A new open science journal rating system has even emerged called the TOP Factor. The idea is not that open science articles are more trustworthy that other types of articles – the idea is that trust doesn’t play a role. Anyone can inspect the data using the scripts and data provided by authors. It’s really just the same as making your science available for auditing the way financial records can be audited. But just like in the world of business some people don’t like the idea of make it possible for others to audit their work. The problems reported at Molecular Brain (doubtless is common to many journals) are likely avoided with open science - because the data and scripts needed to reproduce the numbers in the articles are uploaded prior to publication. The TOP open science guidelines have made an impact and some newer journals, such as Meta Psychology, have fully embraced open science. Figure 3.1 shows the header from an article in Meta Psychology that clearly delineates the open science attributes of the article that used computer simulations (instead of participant data). Take note that the header even specifies who checked that the analyses in the article were reproducible. FIGURE 3.1: Open science in an article header In Canada, the majority of university research is funded by the Federal Government’s Tri-Agency (i.e., NSERC, SSHRC, CIHR). The agency has a new draft Data Management Policy in which they state that “The agencies believe that research data collected with the use of public funds belong, to the fullest extent possible, in the public domain and available for reuse by others.” The perspective of the funding agency on data ownership differs substantially from that of some researchers who incorrectly believe “they own their data”. In Canada at least, the government makes it clear that when tax payers fund research (through the Tri-Agency) the research data is public property. Additionally the Tri-Agency Data Management policy clearly indicates the responsibilities of funded researchers: \"Responsibilities of researchers include: incorporating data management best practices into their research; developing data management plans to guide the responsible collection, formatting, preservation and sharing of their data throughout the entire life cycle of a research project and beyond; following the requirements of applicable institutional and/or funding agency policies and professional or disciplinary standards; acknowledging and citing data sets that contribute to their research; and staying abreast of standards and expectations of their disciplinary community.\" As a result of this perspective on data, it’s important that you think about structuring your data for reuse by yourself and others before you collect it. Toward this end, properly documenting your data file and analysis scripts is critical. 3.4 Begin with the end in mind In this chapter we will walk you though the steps from data collection, data entry, loading raw data, and the creation of data you will analyze (analytic data) via pre-processing scripts. These steps are outlined in Figure 3.2. This figure makes a clear distinction between raw data and analytic data. Raw data refers to the data as you entered it into a spreadsheet or received it from survey software. Analytic data is the data that has been structured and processed so that it is ready for analysis. This pre-processing could include such things as identifying categorical variables to the computer, averaging multiple items into a scale scale scores, and other tasks. It’s critical that you don’t think of the analysis of your data as being completely removed from the data collection and data entry choices you make. Poor choices at the data collection and data entry stage can make your life substantially more complicated when it comes time to write the preprocessing script that will convert your raw data to analytic data. The mantra of this chapter is begin with the end in mind. FIGURE 3.2: Data science pipeline by Roger Peng. It’s difficult to being with the end in mind when you haven’t read later chapters. So here we will be provide you with some general thoughts around different approaches to structuring data files and the naming conventions you use when creating those data files. 3.4.1 Structuring data: Obtaining tidy data When conducting analyses in R it is typically necessary to have data in a format called tidy data (Wickham 2014). tidy data as defined by Hadley involves (among other requirements) that: Each variable forms a column. Each observation forms a row. The tidy data format can be initially challenging for some researchers to understand because it is based on thinking about, and structuring data, in terms of observations/measurements instead of participants. In this section we will describe common approaches to entering animal and human participant data and how they can be done keeping the tidy data requirement in mind. It’s not essential that data be entered in a tidy data format but it is essential that you enter data in manner that makes it easy to later convert data to a tidy data format. When dealing with animal or human participant data it’s common to enter data into a spreadsheet. Each row of the spreadsheet is typically used to represent a single participant and each column of the spreadsheet is used to represent a variable. Between participant data. Consider Table 3.1 which illustrates between participant data for six human participants running 5 kilometers. The first column is id, which indicates there are six unique participants and provides and identification number for each of them. The second column is sex, which is a variable, and there is one observation per for row, so sex also conforms to the tidy data specification. Finally, there is a last column five_km_time which is a variable with one observation per row – also conforming to tidy data specification. Thus, single occasion between subject data like this conforms to the tidy data specification. There is usually nothing you need to do to convert between participant data (or cross-sectional data) to be in a tidy data format. TABLE 3.1: Between participant data entered one row per participant id sex elapsed_time 1 male 40 2 female 35 3 male 38 4 female 33 5 male 42 6 female 36 Within participant data. Consider Table 3.2 which illustrates within participant data for six human participants running 5 kilometers - but on three different occasions. The first column is id, which indicates there are six unique participants and provides and identification number for each of them. The second column is sex, which is a variable, and there is one observation per for row, so sex also conforms to the tidy data specification. Next, there are three different columns (march, may, july) representing each of which contains and elapsed_time for the runner in a different month. Elapsed run times are spread out over three columns so elapse_timne is not in a tidy data format. Moreover, it’s not clear from the data file that march, may, and july are levels of a variable called occasion. Nor is it clear that elapsed_times are recorded in each of those columns (i.e., the dependent variable is unknown/not labelled). Although this format is fine as a data entry format it clearly has problems associated with it when it comes time to analyze your data. TABLE 3.2: Within participant data entered one row per participant id sex march may july 1 male 40 37 35 2 female 35 32 30 3 male 38 35 33 4 female 33 30 28 5 male 42 39 37 6 female 36 33 31 TABLE 3.3: A tidy data version of the within participant data id sex occasion elapsed_time 1 male march 40 1 male may 37 1 male july 35 2 female march 35 2 female may 32 2 female july 30 3 male march 38 3 male may 35 3 male july 33 4 female march 33 4 female may 30 4 female july 28 5 male march 42 5 male may 39 5 male july 37 6 female march 36 6 female may 33 6 female july 31 Thus, a major problem with entering repeated measures data in the one row per person format is that there are hidden variables in the data and you need insider knowledge to know what the columns represent. That said, this is not necessarily a terrible way to enter your data as long as you have all of this missing information documented in a data code book. Disadvantages one row per participant Advantages one row per participant 1) Predictor variable (occasion) is hidden and spread over multiple columns 1) Easy to enter this way 2) Unclear that each month is a level of the predictor variable occasion 3) Dependent variable (elapsed_time) is not indicated 4) Unclear that elapsed_time is the measurement in each month column Fortunately, the problems with Table 3.2 can be largely resolved by converting the data to the a tidy data format. This can be done with the pivot_long() command that we will learn about later in this chapter. Thus, we can enter the data in the format of Table 3.2 and later convert it to a tidy data format. After this conversion the data will be appear as in Table 3.3. For elapsed_time variable this data is now in the tidy data format. Each row corresponds to a single elapsed_time observed. Each column corresponds to a single variable. Somewhat problematically, however, sex is repeated three times for each person (i.e., over the three rows) - and this can be confusing. However, if the focus in on analyzing elapsed time this tidy data format makes sense. Importantly, there is an id column for each participant so R knows that this information is repeated for each participant and is not confused by repeating the sex designation over three rows. In directly, this illustrates the importance of having an id column to indicate each unique participant. Why did we walk you through this technical treatment of structuring data at this point in time? So that you pay attention to the advice the follows. You can see at this point that you may well need to restructure your data for certain analyses. The ability to do so quickly and easily depends upon you following the advice in this chapter around naming conventions for variables and other aspects of your analyses. You can imagine the challenges for converting the data in Figure 3.2 to the data in Figure 3.3 by hand. You want to be able to automate that process and others - which is made substantially easier if you following the forthcoming advice about naming conventions in the tidyverse. 3.5 Data collection considerations Data can be collected in a wide variety of way. Regardless of the methods of location researchers typically come to data in one of two way: 1) a research assistant enters the data into a spreadsheet type interface, or 2) the data is obtained as the output from computer software (e.g., Qualtrics, SurveyMonkey, Noldus, etc.). Regardless of the approach it is critical to name your variables appropriately. For those using software, such as Qualtrics, this means setting up the software to use appropriate variable names PRIOR to data collection - so the exported file has desirable column names. For spreadsheet users, this means setting up the spreadsheet the data will be recorded in with column names that are amenable to the future analyses you want to conduct. Although failure to take this thoughtful approach at the data collection stage can be overcome - it is only overcome with substantial manual effort. Therefore, as noted previously, we strongly encourage you to following the naming conventions we espouse here where you set up your data recording regime. Additionally, we encourage you to give careful thought in advance to the codes you will use to record missing data. 3.5.1 Naming conventions To make your life easier down the road, it is critical you set up your spreadsheet or online survey such that is uses a naming convention prior to data collection. The naming conventions suggested here are adapted from the tidyverse style guide. Lowercase letters only If two word column names are necessary, only use the underscore (\"_\") character to separate words in the name. Avoid short uncontextualized variable names like q1, q2, q3, etc. Do use moderate length column names. Aim to achieve a unique prefix for related columns so that those columns can be selected using the starts_with() command discussed in the previous chapter. Be sure to avoid short two or three letter prefixes for item names. Use moderate length unique item prefixes so that it will easy to select with those columns using start_with() such that you don’t accidentally get additionally columns you don’t want - that have a similar prefix. See the Likert-type item section below for additional details. If you have a column name that represents the levels of two repeated measures variables only use the underscore character to separate the levels of the different variables. See within-participant ANOVA section below for details. 3.5.2 Likert-type items A Likert-type item is typically composed of a statement that participants are asked to agree or disagree with. For example, participants could be asked to indicate the extent to which they agree a number of statements such as “I like my job”. They would then be presented with response scale such as: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. A common question is how should I enter the data? Enter numeric responses not the labels. You should enter the numeric value for each item response (e.g., 5) into your data - not the label (e.g., Strongly Agree). The labels associated with each value can be applied later in a script, if needed. High numbers should be associated with more of the construct being measured. When designing your survey or data collection tools, it is important that you set of the response options appropriately. If your scale measures job satisfaction, it is important that you collect data in a manner that ensures high numbers on the job satisfaction scale indicate high levels of job satisfaction. Therefore, assigning numbers makes sense using the 5-point scale: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. With this approach high response numbers indicate more job satisfaction. However, using the opposite scale would not make sense: 1 - Strongly Agree, 2 - Moderately Agree, 3 - Neutral, 4, Moderately Disagree, 5 - Strongly Disagree. With this opposite scale high numbers on a job satisfaction scale would indicate lower levels of job satisfaction - a very confusing situation. Avoid this situation, assign numbers so that higher numbers are associated with more of the construct being measured. Use appropriate item names. As described in the naming convention section, use moderate length names with different labels for each subscale. Use moderate length column names unique to each subscale. Imagine you have a survey with an 18-item commitment scale (Meyer, Allen, and Smith 1993) composed of three 6-item subscales: affective, normative, and continuance commitment. It would be a poor choice to prefix the labels of all 18 columns in your data with “commit” such that the names would be commit1, commit2, commit3, etc. The problem with this approach is that it fails to distinguishing between the three subscales in naming convention; making it impossible to select the items for a single subscale using starts_with(). A better, but still poor choice for a naming convention would be use use a two letter prefix for the three scale such ac, nc, and cc. This would result in names for the columns like ac1, ac2, ac3, etc. This is an improvement because you could apparently (but likely not) select the columns using starts_with(“ac”). The problem with these short names is that there could be many columns in data set that start with “ac” beside the affective commitment items. You might want to select the affective commitment items using starts_with(“ac”); but you would get all the affective commitment item columns but also all the columns measuring other variables that also start with “ac”. Therefore, it’s a good idea to use a moderate length unique prefix for column names. For example, you might use prefixes like affectcom, normcom, and contincom for the three subscales. This would create column names like affectcom1, affectcom2, affectcom3, etc. These column prefixes are unlikely to be duplicated in other places in your column name conventions making it easy to select those columns using a command like starts_with(“affectcom”). Indicate in the item name if the item is reversed keyed Sometimes with Likert-type items, an item is reverse keyed. For example, on a job satisfaction scale participants will typically respond to items that reflect job satisfaction using the scale: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. Higher numbers indicate more job satisfaction. Sometimes however, some items will use the same 1 to 5 response scale but be worded “I hate my job”. Responding with a 5 to this item would indicate high job dissatisfaction not high job satisfaction - to the response will need to be flipped in your analysis script after data collection (i.e., 1 need to become a 5 and vice versa). To make it easier to do so, you should indicate if an item is reverse keyed in the item name. The procedure for doing so is outlined in the next point. Indicate in the item name the range for reverse key items. If an item is reverse-keyed, the process for the flipping the scores depends upon the range of a scale. Although 5-point scale are common, any number of points are possible. The process for correcting a reverse key item depends upon: 1) the number of points on the scale, and 2) the range of the points on the scale. The reverse-key item correction process is different for an item that uses a 5-point scale ranging from 1 to 5 and from 0 to 4. Both are 5-point scale but your correction process will be different. Therefore, for reverse key items add as suffix at the end of each item name that indicates an item is reverse keyed and the range of the item. For example, if the third job satisfaction item was reversed keyed on scale using a 1 to 5 response format you might name the item: jobsat3_rev15. The suffix \"_rev15\" indicates the item is reverse keyed and the range of responses used on the item is 1 to 5. Be sure to set up your survey with this naming convention when you collect your data. If you collect items over multiple time points use a prefix with a short code to indicate the time followed by and underscore. For example, if you had a multi-item self-esteem scale you might call the column for the firs time “t1_esteem1_rev15”. This indicate that you have for time 1 (t1), the first self-esteem item (esteem1) and that item is reverse keyed on a 1 to 5 scale. 3.5.3 ANOVA between Avoid numerical representation of categorical variables. Don’t use 1 or 2 to represent a variable like sex. Use male and female in your spreadsheet - likewise in your survey program. Similarily, for between participant variables like drug_condition don’t use 1 or 2 use “drug” and “placebo” but the actually drug name would be even better than the word “drug.” 3.5.4 ANOVA within If you have a study that involves within-participant predictors naming conventions can become exomplex. When you have a single repeated measures predictor like occasion in the previous running example, it is often necessary to spread the level of that varible over multiple columns (e.g., march, may, july). When you have multiple repeated measures predictor the situation is even more complicated. In this case, each column name needs to represent the levels of mutiple repeated measures predictors at the time of data entry. For example, imagine you are a food researcher interested in taste ratings (the dependent variable) for various foods and contexts. You have food type (i.e., food_type) as a predictor with three levels (pizza, steak, burger). You have a second predictor temperature with two levels (hot, cold). All participants taste all foods at all temperatures. Thus, six columns are required to record taste rating for each participant: pizza_hot, pizza_cold, steak_hot, steak_cold, burger_hot, and burger_cold. Notice how each name contains one level of each predictor variable. The levels by the two predictor variables are separated by a single underscore. This should be the only underscore in the variable name because that underscore will be used by the computer when changing the data to the tidy format. If you had two underscores an name like “italian_pizza_hot” you would confuse the pivot_longer() command when it attempts to create a tidy version of the data. The computer would think there were three repeated levels variables instead of two. Thus, when dealing with repeated measures predictors, only use underscores to separate levels of predictor variables in column names. 3.6 Following the examples Below we present example scripts transforming raw data to analytic data for various study designs (experimental and survey). These examples illustrate the value of using the naming conventions outlined previously. Don’t just read the example - follow along with the projects by creating a separate script for each example. Resist the urge to cut and paste from this document - type the script yourself. When first learning iPhone/Mac software development, I did so by taking a course at Big Nerd Ranch - yes, that’s a real place. They advised in their material (and now book) the following: “We have learned that “going through the motions” is much more important than it sounds. Many times we will ask you to start typing in code before you understand it. We realize that you may feel like a trained monkey typing in a bunch of code that you do not fully grasp. But the best way to learn coding is to find and fix your typos. Far from being a drag, this basic debugging is where you really learn the ins and outs of the code. That is why we encourage you to type in the code yourself. You could just download it, but copying and pasting is not programming. We want better for you and your skills.\", p. xiv, (Keur and Hillegass 2020). This is excellent advice for a beginning statistician or data scientist as well. And as an aside: if you want to learn iPhone programming you can’t go wrong the Big Nerd Ranch guide! As you work through this chapter, create your own new script for each example. In light of the above advice, avoid copying and pasting code - type it out; you will be the better for it. Getting started: The Class: R Studio in the Cloud Assignment The data should be in the assignment project automatically. Just start the assignment. For everyone in the class, that’s it. For those of you reading this work not in the class, see the two options below: R Studio Cloud, custom project Create a new Project using the web interface Upload all the example data files in to the project. The data files need are listed at the beginning of this chapter. The upload button can be found on the Files tab. R Studio Computer, custom project Create a folder on your computer for the example Place all the example data files in that folder. The data files need are listed at the beginning of this chapter. Use the menu item File &gt; New Project… to start the project On the window that appears select “Existing Directory” On the next screen, press the “Browse” button and find/select the folder with your data Press the Create Project Button Regardless of whether your are working from the cloud or locally you should now have an R Studio project with your data files in it. We ancticipate many people will doubtless want to refer back to an encapsulated set of instructions for each design. There for the example for each design is written in a way that it stands alone. A consequence of this approach is that there is some redundancy in the code across example. We see this a strength - because readers will see the commonalities across differ types of designs. As you make a script for each example: Recall the instruction from Chapter 1 about putting the date and your name in the script via comments. Recall the instruction from Chapter 1 about running library(tidyverse) before you type the rest of each script - this provides you with tidyverse autocomplete for the script. 3.7 Entering data into spreadsheets The first example uses a data file data_ex_between.csv that corresponds to a ficticous example where we recorded the run times for a number of male and female participants. How did we create this data file? We used a spreadsheet to enter the data, as illustrated in Figure 3.3. Programs like Microsoft Excel and Google Sheets are good options for entering data. FIGURE 3.3: Spreadsheet entry of running data The key to using these types of programs is to the save the data as a .csv file when you are done. CSV is short for Comma Separated Values. After entering the data in Figure 3.3 we saved it as data_ex_between.csv. There is no need to do so, but if you were to open this file in a text editor (such as TextEdit on a Mac or Notepad on Windows) you would see the information displayed in Figure 3.4. You can see there is one row per person and the columns are created by separating each values by a comma; hence, comma separated values. FIGURE 3.4: Text view of CSV data There are many ways to save data, but the CSV data is one of the better ones because it is a non-proprietary format. Some software, such as SPSS, uses a proprietary format (e.g., .sav for SPSS) this makes it challenging to access that data if you don’t have that (often expensive) software. One of our goals as scientists is to make it easy for others to audit our work - that allows science to be self-correcting. Therefore, choose an open format for your data like .csv. 3.8 Experiment: Between This section outlines a workflow appropriate for when you plan to conduct independent groups t-tests or a between-particpants ANOVA. To Begin: Use the Files tab to confirm you have the data: data_ex_between.csv Start a new script for this example. Don’t forget to start the script name with “script_”. As noted previously, these data correspond to a design where the researcher is interested in comparing run times (elapsed_time) based on sex (male/female). # Date: YYYY-MM-DD # Name: your name here # Example: Between-participant experiment # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_beween &lt;- read_csv(file = &quot;data_ex_between.csv&quot;, na = my_missing_value_codes) We load the intial data into a raw_data_beween but immediately make a copy we will work with called analytic_data_between. It’s good to keep a copy of the raw data for reference if you encounter problems. analytic_data_between &lt;- raw_data_beween After loading the data we do initial cleaning to remove empty row/columns and ensure proper naming for columns: library(janitor) # Initial cleaning analytic_data_between &lt;- analytic_data_between %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names follow our naming convention with the glimpse() command. glimpse(analytic_data_between) ## Rows: 6 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot; ## $ elapsed_time &lt;dbl&gt; 40, 35, 38, 33, 42, 36 3.8.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect the glimpse() output - if you followed our data entry naming conventions, categorical variables should be of the type character. We have one variable, sex, that is a categorical variable of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. glimpse(analytic_data_between) ## Rows: 6 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot; ## $ elapsed_time &lt;dbl&gt; 40, 35, 38, 33, 42, 36 You can quickly convert all character columns to factors using the code below. In this case, this just converts the sex column to a factor. Because there is only one column (sex) being converted to a factor, we could have treated it the same was as the id column below. However, we use this code because of its broad applicability to many scripts. # Turn all columns that are of type character into factors analytic_data_between &lt;- analytic_data_between %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is a numeric column, so it was not coverted by the above code. The id column is coverted to a factor with the code below. analytic_data_between &lt;-analytic_data_between %&gt;% mutate(id = as_factor(id)) You can ensure both the sex and id columns are now factors using the glimpse() command. glimpse(analytic_data_between) ## Rows: 6 ## Columns: 3 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;fct&gt; male, female, male, female, male, female ## $ elapsed_time &lt;dbl&gt; 40, 35, 38, 33, 42, 36 This example is so small it’s clear you didn’t miss converting any columns to factors. In general, however, at this point you should inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors. 3.8.2 Factor screening Inspect the levels of each factor carefully. Make sure that there not any levels for a factor that are incorrect. For example, you wouldn’t want to the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for eronious factor levels. The code below displays the factor levels: analytic_data_between %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 male :3 ## 2:1 female:3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: female, male. # Custom reordering of factor levels analytic_data_between &lt;- analytic_data_between %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;male&quot;)) You can see the new order of the factor levels with summary(): analytic_data_between %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 female:3 ## 2:1 male :3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 3.8.3 Numeric screening For numeric variables, you should search for impossible values. For example, in the context of this example you want to ensure non of the elapsed_times are impossible or so large they appear to be data entry errors. One option for doing so is the summary command again. This time, however, we use “is.numeric” in the where() command. analytic_data_between %&gt;% select(where(is.numeric)) %&gt;% summary() ## elapsed_time ## Min. :33.0 ## 1st Qu.:35.2 ## Median :37.0 ## Mean :37.3 ## 3rd Qu.:39.5 ## Max. :42.0 Scan the min and max values for to ensure there are not any impossible values. If necessary, got back to the original data source and fix these impossible value. Alternatively, you might need to change them to missing values (i.e., NA values). In this example all the values are reasonable values. However, if we discovered an out of range value (or values) for elapsed time we could convert those values to missing values with the code below. This code changes (i.e., mutates) a value in the elapsed_time column to become NA (not available or missing) if that value is less than zero. If the value is greater than or equal to zero, it stays the same. Note that when using this command we have to be very specific in terms of specifying our missing value. It usually needs to be one of NA_real_ or NA_character_. For numeric columns use NA_real_ and for character columns use NA_character_. analytic_data_between &lt;- analytic_data_between %&gt;% mutate(elapsed_time = case_when( elapsed_time &lt; 0 ~ NA_real_, elapsed_time &gt;= 0 ~ elapsed_time)) Once you are done numeric screening, the data is ready for analysis. References "],
["experiment-within-one-way.html", "Chapter 4 Experiment: Within one-way 4.1 Experiment: Within N-way 4.2 Surveys: Single Occassion 4.3 Surveys: Multiple Occasions 4.4 Basic descriptive statistics", " Chapter 4 Experiment: Within one-way This section outlines a workflow appropriate for when you have a repeated measures design with a single repeated measurs predictor. The data corresponds to a design where the researcher is interested in comparing run times (elapsed_time) across three different occasions (march/may/june). To Begin: Use the Files tab to confirm you have the data: data_ex_within.csv Start a new script for this example. Don’t forget to start the script name with “script_”. # Date: YYYY-MM-DD # Name: your name here # Example: Within-participant experiment # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_within &lt;- read_csv(file = &quot;data_ex_within.csv&quot;, na = my_missing_value_codes) ## Parsed with column specification: ## cols( ## id = col_double(), ## sex = col_character(), ## march = col_double(), ## may = col_double(), ## july = col_double() ## ) We load the intial data into a raw_data_within but immediately make a copy we will work with called analytic_data_within. It’s good to keep a copy of the raw data for reference if you encounter problems. analytic_data_within &lt;- raw_data_within After loading the data we do initial cleaning to remove empty row/columns and ensure proper naming for columns: library(janitor) # Initial cleaning analytic_data_within &lt;- analytic_data_within %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_within) ## Rows: 6 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot; ## $ march &lt;dbl&gt; 40, 35, 38, 33, 42, 36 ## $ may &lt;dbl&gt; 37, 32, 35, 30, 39, 33 ## $ july &lt;dbl&gt; 35, 30, 33, 28, 37, 31 4.0.1 Creating factors Following initial cleaning, we identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect the glimpse() output - if you followed our data entry naming conventions, categorical variables should be of the type character. We have one variable, sex, that is a categorical variable of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. glimpse(analytic_data_within) ## Rows: 6 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot; ## $ march &lt;dbl&gt; 40, 35, 38, 33, 42, 36 ## $ may &lt;dbl&gt; 37, 32, 35, 30, 39, 33 ## $ july &lt;dbl&gt; 35, 30, 33, 28, 37, 31 You can quickly convert all character columns to factors using the code below. In this case, this just converts the sex column to a factor. Because there is only one column (sex) being converted to a factor, we could have treated it the same was as the id column below. However, we use this code because of its broad applicability to many scripts. # Turn all columns that are of type character into factors analytic_data_within &lt;- analytic_data_within %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is a numeric column, so it was not coverted by the above code. The id column is coverted to a factor with the code below. analytic_data_within &lt;-analytic_data_within %&gt;% mutate(id = as_factor(id)) You can ensure both the sex and id columns are now factors using the glimpse() command. glimpse(analytic_data_within) ## Rows: 6 ## Columns: 5 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;fct&gt; male, female, male, female, male, female ## $ march &lt;dbl&gt; 40, 35, 38, 33, 42, 36 ## $ may &lt;dbl&gt; 37, 32, 35, 30, 39, 33 ## $ july &lt;dbl&gt; 35, 30, 33, 28, 37, 31 This example is so small it’s clear you didn’t miss converting any columns to factors. In general, however, at this point you should inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors. 4.0.2 Factor screening Inspect the levels of each factor carefully. Make sure that there not any levels for a factor that are incorrect. For example, you wouldn’t want to the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factors in your data for eronious factor levels. The code below displays the factor levels: analytic_data_within %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 male :3 ## 2:1 female:3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: female, male. # Custom reordering of factor levels analytic_data_within &lt;- analytic_data_within %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;male&quot;)) You can see the new order of the factor levels with summary(): analytic_data_within %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 female:3 ## 2:1 male :3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 4.0.3 Numeric screening For numeric variables, you should search for impossible values. For example, in the context of this example you want to ensure non of the elapsed_times are impossible or so large they appear to be data entry errors. One option for doing so is the summary command again. This time, however, we use “is.numeric” in the where() command. library(skimr) analytic_data_within %&gt;% skim() TABLE 4.1: Data summary Name Piped data Number of rows 6 Number of columns 5 _______________________ Column type frequency: factor 2 numeric 3 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts id 0 1 FALSE 6 1: 1, 2: 1, 3: 1, 4: 1 sex 0 1 FALSE 2 fem: 3, mal: 3 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist march 0 1 37.33 3.33 33 35.25 37 39.5 42 ▃▇▃▃▃ may 0 1 34.33 3.33 30 32.25 34 36.5 39 ▃▇▃▃▃ july 0 1 32.33 3.33 28 30.25 32 34.5 37 ▃▇▃▃▃ ## skim_variable n_missing mean sd p0 p50 p100 ## 1 march 0 37.33 3.33 33 37 42 ## 2 may 0 34.33 3.33 30 34 39 ## 3 july 0 32.33 3.33 28 32 37 Scan the min and max values for to ensure there are not any impossible values. If necessary, got back to the original data source and fix these impossible value. Alternatively, you might need to change them to missing values (i.e., NA values). In this example all the values are reasonable values. However, if we discovered an out of range value (or values) for elapsed time we could convert those values to missing values with the code below. This code changes (i.e., mutates) a value in the elapsed_time column to become NA (not available or missing) if that value is less than zero. If the value is greater than or equal to zero, it stays the same. Note that when using this command we have to be very specific in terms of specifying our missing value. It usually needs to be one of NA_real_ or NA_character_. For numeric columns use NA_real_ and for character columns use NA_character_. analytic_data_between &lt;- analytic_data_between %&gt;% mutate(elapsed_time = case_when( elapsed_time &lt; 0 ~ NA_real_, elapsed_time &gt;= 0 ~ elapsed_time)) 4.0.4 Pivot to tidy data The analytic data in it’s current for does not conform to the the tidy data specification. Inspect the data with the print() command. Notice the there is not column for occasion. The levels of occasion are spread across three columns. Correspondingly, elapsed time is spread across multiple columns in the same row and is not labeled as elaped time. We need to restructure the data into the tidy data format so that we have a single elapsed time observation per row and a single column per variable. print(analytic_data_within) ## # A tibble: 6 x 5 ## id sex march may july ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 male 40 37 35 ## 2 2 female 35 32 30 ## 3 3 male 38 35 33 ## 4 4 female 33 30 28 ## 5 5 male 42 39 37 ## 6 6 female 36 33 31 The pivot_longer() command below coverts our data to the tidy data format. In this command we specify the the columns march, may, and june are all levels of a single varible called occasion. We specify the columns involved with the cols argument. The code march:july after the cols argument indicates to the computer to use the march column and the july column and all the columns in between. Each column name represents a level of the variable occasion. The names_to argument is used to indicate that a new column called occasion should be created to hold the different months. The value_to argument is used to indicate that a new column called elapsed_time should created to hold all the values from the march, may, and july column. analytic_data_within_tidy &lt;- analytic_data_within %&gt;% pivot_longer(cols = march:july, names_to = &quot;occasion&quot;, values_to = &quot;elapsed_time&quot; ) You can see the data in the new format below. print(analytic_data_within_tidy) ## # A tibble: 18 x 4 ## id sex occasion elapsed_time ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 male march 40 ## 2 1 male may 37 ## 3 1 male july 35 ## 4 2 female march 35 ## 5 2 female may 32 ## 6 2 female july 30 ## 7 3 male march 38 ## 8 3 male may 35 ## 9 3 male july 33 ## 10 4 female march 33 ## 11 4 female may 30 ## 12 4 female july 28 ## 13 5 male march 42 ## 14 5 male may 39 ## 15 5 male july 37 ## 16 6 female march 36 ## 17 6 female may 33 ## 18 6 female july 31 But notice that the new column occasion is of the type character. We need it to be a factor. So use the code below to do so: analytic_data_within_tidy &lt;-analytic_data_within_tidy %&gt;% mutate(occasion = as_factor(occasion)) You can confirm that occasion is now a factor with the glimpse() command. Once this is complete, you are done preparing your one-way within participant analytic data. glimpse(analytic_data_within_tidy) ## Rows: 18 ## Columns: 4 ## $ id &lt;fct&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6 ## $ sex &lt;fct&gt; male, male, male, female, female, female, male, male, … ## $ occasion &lt;fct&gt; march, may, july, march, may, july, march, may, july, … ## $ elapsed_time &lt;dbl&gt; 40, 37, 35, 35, 32, 30, 38, 35, 33, 33, 30, 28, 42, 39… You now have two data files… for different things.. Once you are done numeric screening, the data is ready for analysis. 4.1 Experiment: Within N-way The N-way within-participant ANOVA example uses the data file data_food.csv. Initial loading of the file, including the specification of missing data codes, is provided by the code below. The data corresponds to a designs where the researcher is interested in assessing the taste of food as a function of food type (pizza/steak/burger) and temperature (hot/cold). # Date: YYYY-MM-DD # Name: your name here # Example: Within-participant experiment # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_within_nway &lt;- read_csv(file = &quot;data_food.csv&quot;, na = my_missing_value_codes) analytic_data_within_nway &lt;- raw_data_within_nway After loading the data we do initial cleaning to remove empty row/columns and ensure proper naming for columns: library(janitor) # Initial cleaning analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_within_nway) ## Rows: 6 ## Columns: 8 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;dbl&gt; 1, 2, 1, 2, 1, 2 ## $ pizza_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 9 ## $ pizza_cold &lt;dbl&gt; 6, 7, 5, 7, 6, 7 ## $ steak_hot &lt;dbl&gt; 6, 6, 7, 7, 8, 7 ## $ steak_cold &lt;dbl&gt; 3, 3, 4, 5, 7, 8 ## $ burger_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 8 ## $ burger_cold &lt;dbl&gt; 4, 3, 3, 3, 2, 5 4.1.1 Creating factors Following initial cleaning, identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect glimpse() output - if you followed our data entry naming conventions categorical variables should be of the type character. We have one variable sex that is categorical that is of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column. glimpse(analytic_data_within_nway) ## Rows: 6 ## Columns: 8 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;dbl&gt; 1, 2, 1, 2, 1, 2 ## $ pizza_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 9 ## $ pizza_cold &lt;dbl&gt; 6, 7, 5, 7, 6, 7 ## $ steak_hot &lt;dbl&gt; 6, 6, 7, 7, 8, 7 ## $ steak_cold &lt;dbl&gt; 3, 3, 4, 5, 7, 8 ## $ burger_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 8 ## $ burger_cold &lt;dbl&gt; 4, 3, 3, 3, 2, 5 The sex column is a numeric column, so we have handle that column on it’s own. analytic_data_within_nway &lt;-analytic_data_within_nway %&gt;% mutate(sex = as_factor(sex)) Now the sex column is a factor but we have to tell the computer that 1 indicates male and 2 indicates female. analytic_data_within_nway &lt;-analytic_data_within_nway %&gt;% mutate(sex = fct_recode(sex, male = &quot;1&quot;, female = &quot;2&quot;)) The participant identification number in the id column is a numeric column, so we have handle that column on it’s own. analytic_data_within_nway &lt;-analytic_data_within_nway %&gt;% mutate(id = as_factor(id)) You can ensure all of these columns are now factors using the glimpse() command. glimpse(analytic_data_within_nway) ## Rows: 6 ## Columns: 8 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6 ## $ sex &lt;fct&gt; male, female, male, female, male, female ## $ pizza_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 9 ## $ pizza_cold &lt;dbl&gt; 6, 7, 5, 7, 6, 7 ## $ steak_hot &lt;dbl&gt; 6, 6, 7, 7, 8, 7 ## $ steak_cold &lt;dbl&gt; 3, 3, 4, 5, 7, 8 ## $ burger_hot &lt;dbl&gt; 7, 8, 7, 8, 7, 8 ## $ burger_cold &lt;dbl&gt; 4, 3, 3, 3, 2, 5 Inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors. 4.1.2 Factor screening Inspect the levels of each factor carefully. Make sure that there not any levels present that are incorrect. For example, you wouldn’t want to the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factor in your data for eronious factor levels. The code below displays the factor levels: analytic_data_within_nway %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 male :3 ## 2:1 female:3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how text output and graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: female, male. # Custom reordering of factor levels analytic_data_within_nway &lt;- analytic_data_within_nway %&gt;% mutate(sex = fct_relevel(sex, &quot;female&quot;, &quot;male&quot;)) You can see the new order of the factor levels with summary(): analytic_data_within_nway %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex ## 1:1 female:3 ## 2:1 male :3 ## 3:1 ## 4:1 ## 5:1 ## 6:1 4.1.3 Numeric screening For numeric variables, you should search for impossible values.e For example, in the context of this example you want to ensure non of the elapsed_times are impossible or so large they appear to be data entry errors. One options for doing so is the summary command again. This time, however, we use “is.numeric” in the where() command. analytic_data_within_nway %&gt;% select(where(is.numeric)) %&gt;% summary() ## pizza_hot pizza_cold steak_hot steak_cold burger_hot ## Min. :7.00 Min. :5.00 Min. :6.00 Min. :3.00 Min. :7.0 ## 1st Qu.:7.00 1st Qu.:6.00 1st Qu.:6.25 1st Qu.:3.25 1st Qu.:7.0 ## Median :7.50 Median :6.50 Median :7.00 Median :4.50 Median :7.5 ## Mean :7.67 Mean :6.33 Mean :6.83 Mean :5.00 Mean :7.5 ## 3rd Qu.:8.00 3rd Qu.:7.00 3rd Qu.:7.00 3rd Qu.:6.50 3rd Qu.:8.0 ## Max. :9.00 Max. :7.00 Max. :8.00 Max. :8.00 Max. :8.0 ## burger_cold ## Min. :2.00 ## 1st Qu.:3.00 ## Median :3.00 ## Mean :3.33 ## 3rd Qu.:3.75 ## Max. :5.00 Scan the min and max values for each variable to ensure there are not any impossible values. If necessary, got back to the original data source and fix these impossible value. Alternatively, you might need to change them to missing values (i.e., NA values). Once this is complete, you are done preparing your between participant analytic data. 4.1.4 Pivot to tidy data The analytic data in it’s current for does not conform to the the tidy data specification. Inspect the data with the print() command. Notice the there is not column for occasion. The levels of occasion are spread across three columns. Correspondingly, elapsed time is spread across multiple columns in the same row and is not labeled as elaped time. We need to restructure the data into the tidy data format so that we have a single elapsed time observation per row and a single column per variable. print(analytic_data_within_nway) ## # A tibble: 6 x 8 ## id sex pizza_hot pizza_cold steak_hot steak_cold burger_hot burger_cold ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 male 7 6 6 3 7 4 ## 2 2 fema… 8 7 6 3 8 3 ## 3 3 male 7 5 7 4 7 3 ## 4 4 fema… 8 7 7 5 8 3 ## 5 5 male 7 6 8 7 7 2 ## 6 6 fema… 9 7 7 8 8 5 The pivot_long() command below coverts our data to the tidy data format. In this command we specify the the columns march, may, and june are all levels of a single varible called occasion. We specify the columns involved with the cols argument. The code march:july after the cols argument indicates to the computer to use the march column and the july column and all the columns in between. Each column name represents a level of the variable occasion. The names_to argument is used to indicate that a new column called occasion should be created to hold the different months. The value_to argument is used to indicate that a new column called elapsed_time should created to hold all the values from the march, may, and july column. analytic_data_nway_tidy &lt;- analytic_data_within_nway %&gt;% pivot_longer(pizza_hot:burger_cold, names_to = c(&quot;food&quot;, &quot;temperature&quot;), names_sep = &quot;_&quot;, values_to = &quot;taste&quot; ) You can see the data in the new format below. print(analytic_data_nway_tidy) ## # A tibble: 36 x 5 ## id sex food temperature taste ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 male pizza hot 7 ## 2 1 male pizza cold 6 ## 3 1 male steak hot 6 ## 4 1 male steak cold 3 ## 5 1 male burger hot 7 ## 6 1 male burger cold 4 ## 7 2 female pizza hot 8 ## 8 2 female pizza cold 7 ## 9 2 female steak hot 6 ## 10 2 female steak cold 3 ## # … with 26 more rows But notice that the new column occasion is of the type character. We need it to be a factor. So use the code below to do so: analytic_data_nway_tidy &lt;-analytic_data_nway_tidy %&gt;% mutate(food = as_factor(food), temperature = as_factor(temperature)) You can confirm that occasion is now a factor with the glimpse() command. Once this is complete, you are done preparing your one-way within participant analytic data. glimpse(analytic_data_nway_tidy) ## Rows: 36 ## Columns: 5 ## $ id &lt;fct&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4… ## $ sex &lt;fct&gt; male, male, male, male, male, male, female, female, fem… ## $ food &lt;fct&gt; pizza, pizza, steak, steak, burger, burger, pizza, pizz… ## $ temperature &lt;fct&gt; hot, cold, hot, cold, hot, cold, hot, cold, hot, cold, … ## $ taste &lt;dbl&gt; 7, 6, 6, 3, 7, 4, 8, 7, 6, 3, 8, 3, 7, 5, 7, 4, 7, 3, 8… 4.2 Surveys: Single Occassion Even assuming you did everything right there is still a fair amount of work to get things started paragraph here… We begin by examining the data as originally entered into a spreadsheet. In Figure 4.1 you see a screen shot of the initial raw data as a researcher might receive it. Take careful note of the numerous -999 values used to indicate missing values. As part of creating the analytic data that we will analyze we need to indicate to the computer that the -999 are not data but codes to represent missing values. FIGURE 4.1: Raw data for item scoring Create a new script in your project and save it with the name “script_raw_to_analytic.R” Type the code below into that script # Date: YYYY-MM-DD # Name: your name here # Example: single occasion survey # Load data library(tidyverse) my_missing_value_codes &lt;- c(&quot;-999&quot;, &quot;&quot;, &quot;NA&quot;) raw_data_survey &lt;- read_csv(file = &quot;data_item_scoring.csv&quot;, na = my_missing_value_codes) ## Parsed with column specification: ## cols( ## id = col_double(), ## age = col_double(), ## sex = col_character(), ## eye_color = col_character(), ## esteem1 = col_double(), ## esteem2 = col_double(), ## esteem3 = col_double(), ## esteem4 = col_double(), ## esteem5_rev15 = col_double(), ## jobsat1 = col_double(), ## jobsat2_rev15 = col_double(), ## jobsat3 = col_double(), ## jobsat4 = col_double(), ## jobsat5 = col_double() ## ) analytic_data_survey &lt;- raw_data_survey Remove empty row and columns from your data using the remove_empty_cols() and remove_empty_rows(), respectively. As well, clean the names of your columns to ensure they conform to tidyverse naming conventions. library(janitor) # Initial cleaning analytic_data_survey &lt;- analytic_data_survey %&gt;% remove_empty(&quot;rows&quot;) %&gt;% remove_empty(&quot;cols&quot;) %&gt;% clean_names() You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24, 1… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;,… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue&quot;, NA, &quot;hazel&quot;, &quot;blue&quot;… ## $ esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, 3, … ## $ esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2, 2… ## $ esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA, NA… ## $ esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, 3, … ## $ esteem5_rev15 &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 3, 2, 2, 3, 3, NA, 3,… ## $ jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3,… ## $ jobsat2_rev15 &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 2, 3… ## $ jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2… ## $ jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5, 4,… ## $ jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, 4, … 4.2.1 Creating factors Following initial cleaning, identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect glimpse() output - if you followed our data entry naming conventions categorical variables should be of the type character glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24, 1… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;,… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue&quot;, NA, &quot;hazel&quot;, &quot;blue&quot;… ## $ esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, 3, … ## $ esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2, 2… ## $ esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA, NA… ## $ esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, 3, … ## $ esteem5_rev15 &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 3, 2, 2, 3, 3, NA, 3,… ## $ jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3,… ## $ jobsat2_rev15 &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 2, 3… ## $ jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2… ## $ jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5, 4,… ## $ jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, 4, … You can quickly convert all character columns to factors using the code below: # Turn all columns that are of type character into factors analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is a numeric column, so we have handle that column on it’s own. analytic_data_survey &lt;-analytic_data_survey %&gt;% mutate(id = as_factor(id)) You can ensure all of these columns are now factors using the glimpse() command. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24, 1… ## $ sex &lt;fct&gt; male, female, male, female, male, female, male, femal… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, blue, brown, haz… ## $ esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, 3, … ## $ esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2, 2… ## $ esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA, NA… ## $ esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, 3, … ## $ esteem5_rev15 &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 3, 2, 2, 3, 3, NA, 3,… ## $ jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3,… ## $ jobsat2_rev15 &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 2, 3… ## $ jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2… ## $ jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5, 4,… ## $ jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, 4, … Inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors. 4.2.2 Factor screening Inspect the levels of each factor carefully. Make sure that there not any levels present that are incorrect. For example, you wouldn’t want to the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factor in your data for eronious factor levels. The code below displays the factor levels: analytic_data_survey %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 male :147 blue : 99 ## 2 : 1 female :149 brown: 98 ## 3 : 1 intersex: 2 hazel:100 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how text output and graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: female, male. # Custom reordering of factor levels analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(sex = fct_relevel(sex, &quot;intersex&quot;, &quot;female&quot;, &quot;male&quot;)) For eye color, we want to future graph to be have the most commmon eye colors on the left so we reorder the factor levels: # Reordering factor levels by frequency analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(eye_color = fct_infreq(eye_color)) You can see the new order of the factor levels with summary(): analytic_data_survey %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 intersex: 2 hazel:100 ## 2 : 1 female :149 blue : 99 ## 3 : 1 male :147 brown: 98 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 4.2.3 Numeric screening For numeric variables, you should search for impossible values. For example, in the context of this example you want to ensure none of the elapsed_times are impossible or so large they appear to be data entry errors. One options for doing so is the summary command again. This time, however, we use “is.numeric” in the where() command. However, when a survey has a large number of variables this type of output can become challenging to intrepret. analytic_data_survey %&gt;% select(where(is.numeric)) %&gt;% summary() With larger numers of variables, it’s a good idea to use the skimr package. Additionally, it makes sense to look at sets of items rather than all of the variables at once. This is because sometime items and different response formats. For example, for one measure this responses may range from 1 to 5 whereas for another measure responses could range from 1 to 7. This is undesirable from a psychometric point of view, as discussed previously, but it if happens it makes sense to look at items in sets. Start by examining the range of non-scale items. In this case it’s only age. Examine the output to see if any of the age values are unreasonble. In the output p0 and p100 indicate the 0th percentile and the 100th percentile; that is the min an max values for the variable. Check to make sure none of the age values are unreasonably low or high. If they are, you may need to check the original data source or replace them with missing values. library(skimr) analytic_data_survey %&gt;% select(age) %&gt;% skim_without_charts() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 age 3 20.52 2.05 17 20 24 Look the items in the first scale, self-esteem. Possible items responses for this scale range from 1 to 5, make sure they are all in this range. In the output p0 and p100 indicate the 0th percentile and the 100th percentile; that is the minimum an maximum values for the variable in these data. Examine the output to see if any of the esteem values are unreasonble (i.e., fall outside the 1 to 5 range). If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_survey %&gt;% select(starts_with(&quot;esteem&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 esteem1 24 3.39 0.54 3 3 5 ## 2 esteem2 28 2.35 0.48 2 2 3 ## 3 esteem3 31 3.96 0.37 3 4 5 ## 4 esteem4 15 3.54 0.50 3 4 4 ## 5 esteem5_rev15 35 2.22 0.47 1 2 3 Follow the same process for the job satisfaction items. Possible items responses for this scale range from 1 to 5, make sure they are all in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_survey %&gt;% select(starts_with(&quot;jobsat&quot;)) %&gt;% skim_without_charts() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 jobsat1 25 3.34 0.51 3 3 5 ## 2 jobsat2_rev15 27 1.51 0.61 1 1 3 ## 3 jobsat3 28 2.84 0.37 2 3 3 ## 4 jobsat4 35 4.29 0.70 3 4 5 ## 5 jobsat5 24 4.57 0.61 3 5 5 4.2.4 Scale scores Scale scores involve averaging, for each person, their score over several items to create an overall scale score. The first step in the creation of scales if correcting the values of any reverse-keyed items, as described previously. 4.2.4.1 Reverse key items The way you deal with reverse-keyed items depends on how you scored them. Imagine you had a 5-point scale. You could have scored the scale with the values 1, 2, 3, 4, and 5. Alternatively, you could have scored the scale with the values 0, 1, 2, 3, and 4. In this example, we scored the data using the 1 to 5 system. So we’ll use that. Later I’ll show you how to deal with the other scoring system (0 to 4). We need to take items that were reversed-key when the participant wrote them and recode those responses. We do that with using the mutate command. In this data file all the reverse-keyed were identified with the suffix \"_rev15\" in the column names. This suffix indicates the item was reverse-keyed and that the original scale used the response points 1 to 5. We can see those items with the glimpse() command below. Notice that there are two reverse keyed items - both on difference scales. analytic_data_survey %&gt;% select(ends_with(&quot;_rev15&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 2 ## $ esteem5_rev15 &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 3, 2, 2, 3, 3, NA, 3,… ## $ jobsat2_rev15 &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 2, 3… To correct a reverse-keyed items that ranges where the lowest possible rating value is a 1 (i.e, 1 on a 1 to 5 scale), we simply subtract all the scores from one more than the highest possible rating. For example, if a 1 to 5 response scale was used we subtract each response from 6 to obaiin the recoded value. Original value Math Recoded value 1 6 - 1 5 2 6 - 2 4 3 6 - 3 3 4 6 - 4 2 5 6 - 5 1 The code below: * selects each column that ends with \"_rev15\" (i.e., both esteem and jobsat scales) * subtracts each value in that column from 6 * renames the column by removing \"_rev15\" because the reverse coding is complete. analytic_data_survey &lt;- analytic_data_survey %&gt;% mutate(6 - across(.cols = ends_with(&quot;_rev15&quot;)) ) %&gt;% rename_with(.fn = str_remove, .cols = ends_with(&quot;_rev15&quot;), pattern = &quot;_rev15&quot;) You can use the glimpse() command to see the result of your work. If you compare this to values from the previous glimpse() command you can see they have changed. Also notice the column names not longer indicate the items are reverse-keyed. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24, 17, N… ## $ sex &lt;fct&gt; male, female, male, female, male, female, male, female, m… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, blue, brown, hazel, … ## $ esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, 3, 3, 3… ## $ esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2, 2, 3,… ## $ esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA, NA, 3,… ## $ esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, 3, 3, 4… ## $ esteem5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3, 4, 4, 3, 3, NA, 3, 3, … ## $ jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, … ## $ jobsat2 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, 5, 3, 4, 5, 5, 4, 3, 5,… ## $ jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3,… ## $ jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5, 4, 3, … ## $ jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, 4, 4, 5… If your scale had used response options numbered 0 to 4 the math is different. For each item you would use subtract values from the highest possible point (i.e, 4) instead of one larger than the highest possible point. Original value Math Recoded value 0 4 - 0 4 1 4 - 1 3 2 4 - 2 2 3 4 - 3 1 4 4 - 4 0 Thus the mutate command would instead be: mutate(4 - across(.cols = ends_with(\"_rev15\")) ) 4.2.4.2 Creating scores The process we use for creating scale scores deletes item level data from analytic_data_survey. This is a desirable aspect of the process we it removes information we are no longer interested in from our analytic data. That said, before we create scale score, we create a backup on the item level data called analytic_data_items. We will need to use this backup later to compute the reliability of the scales we are creating. analytic_data_items &lt;- analytic_data_survey We want to make a self_esteem scale and plan to select items using starts_with(“esteem”). But prior to doing this we make sure the start_with() command only gives us the items we want - and not additional unwanted items. The output below confirms there are not problems associated with using starts_with(“esteem”). # confirm select will select the columns you want (and not others) analytic_data_survey %&gt;% select(starts_with(&quot;esteem&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, 3, 3, 3, … ## $ esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2, 2, 3, 2… ## $ esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA, NA, 3, 4… ## $ esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, 3, 3, 4, … ## $ esteem5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3, 4, 4, 3, 3, NA, 3, 3, NA… Likewise, we want to make a job_sat scale and plan to select items using starts_with(“jobsat”). The code and output below using starts_with(“jobsat”) only returns the items we are interested in. # confirm select will select the columns you want (and not others) analytic_data_survey %&gt;% select(starts_with(&quot;jobsat&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, 3,… ## $ jobsat2 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, 5, 3, 4, 5, 5, 4, 3, 5, 4… ## $ jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3, 3… ## $ jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5, 4, 3, 4,… ## $ jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, 4, 4, 5, … We computer the scale scores using the rowwise() command. The mean() command provides the mean of columns by default - not people. We use the rowwise() command in the code below to that the mean() command will work across columns rather than within columns. The mutate comand calculates the scale score for each person. The c_across() command combined with the starts_with() command ensure the items we want averaged together are the items that are averaged together. Notice there is a separate mutate line for each scale. The ungroup() command turns off the rowwise() command. We end the code block by remove the item-level data from the data set. analytic_data_survey &lt;- analytic_data_survey %&gt;% rowwise() %&gt;% mutate(self_esteem = mean(c_across(starts_with(&quot;esteem&quot;)), na.rm = TRUE)) %&gt;% mutate(job_sat = mean(c_across(starts_with(&quot;jobsat&quot;)), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(-starts_with(&quot;esteem&quot;)) %&gt;% select(-starts_with(&quot;jobsat&quot;)) We can see our data now has the self esteem column, a job_sat column, and that all of the item level data has been removed. glimpse(analytic_data_survey) ## Rows: 300 ## Columns: 6 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, … ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24, 17,… ## $ sex &lt;fct&gt; male, female, male, female, male, female, male, female,… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, blue, brown, hazel… ## $ self_esteem &lt;dbl&gt; 3.200, 3.800, 3.800, 3.000, 3.400, 3.500, 3.000, 3.800,… ## $ job_sat &lt;dbl&gt; 4.000, 5.000, 4.400, 3.500, 4.000, 3.800, 3.600, 4.600,… These data are ready for analysis. 4.3 Surveys: Multiple Occasions library(tidyverse) raw_data_occasions &lt;- read_csv(&quot;data_item_time.csv&quot;) analytic_data_occasions &lt;- raw_data_occasions glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 24 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;femal… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue&quot;, NA, &quot;hazel&quot;, &quot;bl… ## $ t1_esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, … ## $ t1_esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2… ## $ t1_esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA,… ## $ t1_esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, … ## $ t1_esteem5_rev15 &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 3, 2, 2, 3, 3, NA,… ## $ t1_jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3,… ## $ t1_jobsat2_rev15 &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 2… ## $ t1_jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2… ## $ t1_jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5,… ## $ t1_jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, … ## $ t2_esteem1 &lt;dbl&gt; 4, 5, 5, 4, NA, 4, 4, 5, 5, 5, 4, 5, 3, 3, 4, 4, 4… ## $ t2_esteem2 &lt;dbl&gt; 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 3, 3, 3, 4, 3, 3, 3,… ## $ t2_esteem3 &lt;dbl&gt; 5, 5, 5, 4, 5, 5, 3, 5, 5, 4, 5, NA, 5, 3, 5, 3, 3… ## $ t2_esteem4 &lt;dbl&gt; 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 3, 5, 4, 4, 5, 3, 4,… ## $ t2_esteem5_rev15 &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 4,… ## $ t2_jobsat1 &lt;dbl&gt; 4, 6, 5, 4, 4, 4, 4, 6, 4, NA, 4, 5, 5, 4, 4, 5, 4… ## $ t2_jobsat2_rev15 &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 4, 2, 4, 3, 2, 2, 3,… ## $ t2_jobsat3 &lt;dbl&gt; 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3,… ## $ t2_jobsat4 &lt;dbl&gt; 3, 6, 6, 5, 5, 5, 5, 6, 3, 5, 3, 6, 5, 5, 5, 6, 5,… ## $ t2_jobsat5 &lt;dbl&gt; 6, 3, 6, 5, NA, 5, 5, 6, 6, 6, 5, 3, 5, 6, 5, 5, 5… 4.3.1 Creating factors Following initial cleaning, identify categorical variables as factors. If you plan to conduct an ANOVA - it’s critical that all predictor variables are converted to factors. Inspect glimpse() output - if you followed our data entry naming conventions categorical variables should be of the type character glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 24 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;femal… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;brown&quot;, &quot;hazel&quot;, &quot;blue&quot;, NA, &quot;hazel&quot;, &quot;bl… ## $ t1_esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, … ## $ t1_esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2… ## $ t1_esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA,… ## $ t1_esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, … ## $ t1_esteem5_rev15 &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 3, 2, 2, 3, 3, NA,… ## $ t1_jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3,… ## $ t1_jobsat2_rev15 &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 2… ## $ t1_jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2… ## $ t1_jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5,… ## $ t1_jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, … ## $ t2_esteem1 &lt;dbl&gt; 4, 5, 5, 4, NA, 4, 4, 5, 5, 5, 4, 5, 3, 3, 4, 4, 4… ## $ t2_esteem2 &lt;dbl&gt; 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 3, 3, 3, 4, 3, 3, 3,… ## $ t2_esteem3 &lt;dbl&gt; 5, 5, 5, 4, 5, 5, 3, 5, 5, 4, 5, NA, 5, 3, 5, 3, 3… ## $ t2_esteem4 &lt;dbl&gt; 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 3, 5, 4, 4, 5, 3, 4,… ## $ t2_esteem5_rev15 &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 4,… ## $ t2_jobsat1 &lt;dbl&gt; 4, 6, 5, 4, 4, 4, 4, 6, 4, NA, 4, 5, 5, 4, 4, 5, 4… ## $ t2_jobsat2_rev15 &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 4, 2, 4, 3, 2, 2, 3,… ## $ t2_jobsat3 &lt;dbl&gt; 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3,… ## $ t2_jobsat4 &lt;dbl&gt; 3, 6, 6, 5, 5, 5, 5, 6, 3, 5, 3, 6, 5, 5, 5, 6, 5,… ## $ t2_jobsat5 &lt;dbl&gt; 6, 3, 6, 5, NA, 5, 5, 6, 6, 6, 5, 3, 5, 6, 5, 5, 5… You can quickly convert all character columns to factors using the code below. In this case, that will convert the sex and eye_colour columns to factors. analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(across(.cols = where(is.character), .fns = as_factor)) The participant identification number in the id column is a numeric column, so we have handle that column on it’s own. analytic_data_occasions &lt;-analytic_data_occasions %&gt;% mutate(id = as_factor(id)) You can ensure all of these columns are now factors using the glimpse() command. glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 24 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24… ## $ sex &lt;fct&gt; male, female, male, female, male, female, male, fe… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, blue, brown, … ## $ t1_esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, … ## $ t1_esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2… ## $ t1_esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA,… ## $ t1_esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, … ## $ t1_esteem5_rev15 &lt;dbl&gt; 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 3, 2, 2, 3, 3, NA,… ## $ t1_jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3,… ## $ t1_jobsat2_rev15 &lt;dbl&gt; 1, 1, 1, NA, 1, 1, 2, 1, 2, 2, 3, 1, 3, 2, 1, 1, 2… ## $ t1_jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2… ## $ t1_jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5,… ## $ t1_jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, … ## $ t2_esteem1 &lt;dbl&gt; 4, 5, 5, 4, NA, 4, 4, 5, 5, 5, 4, 5, 3, 3, 4, 4, 4… ## $ t2_esteem2 &lt;dbl&gt; 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 3, 3, 3, 4, 3, 3, 3,… ## $ t2_esteem3 &lt;dbl&gt; 5, 5, 5, 4, 5, 5, 3, 5, 5, 4, 5, NA, 5, 3, 5, 3, 3… ## $ t2_esteem4 &lt;dbl&gt; 4, 5, 5, 4, 5, 5, 5, 5, 4, 5, 3, 5, 4, 4, 5, 3, 4,… ## $ t2_esteem5_rev15 &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 4,… ## $ t2_jobsat1 &lt;dbl&gt; 4, 6, 5, 4, 4, 4, 4, 6, 4, NA, 4, 5, 5, 4, 4, 5, 4… ## $ t2_jobsat2_rev15 &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 3, 2, 3, 3, 4, 2, 4, 3, 2, 2, 3,… ## $ t2_jobsat3 &lt;dbl&gt; 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3,… ## $ t2_jobsat4 &lt;dbl&gt; 3, 6, 6, 5, 5, 5, 5, 6, 3, 5, 3, 6, 5, 5, 5, 6, 5,… ## $ t2_jobsat5 &lt;dbl&gt; 6, 3, 6, 5, NA, 5, 5, 6, 6, 6, 5, 3, 5, 6, 5, 5, 5… Inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors. 4.3.2 Screening factors Inspect the levels of each factor carefully. Make sure that there not any levels present that are incorrect. For example, you wouldn’t want to the following levels for sex: male, mmale, female. Obviously, mmale is an incorrectly typed version of male. Scan all the factor in your data for eronious factor levels. The code below displays the factor levels: analytic_data_occasions %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 male :147 blue : 99 ## 2 : 1 female :149 brown: 98 ## 3 : 1 intersex: 2 hazel:100 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 Also inspect the output of the above summary() command paying attention to the order of the levels in the factors. The order influences how text output and graphs are generated. In these data, the sex column has two levels: male and female in that order. Below we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: female, male. # Custom reordering of factor levels for sex analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(sex = fct_relevel(sex, &quot;intersex&quot;, &quot;female&quot;, &quot;male&quot;)) For eye color, we want to future graph to be have the most commmon eye colors on the left so we reorder the factor levels: # Reordering factor levels by frequency analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(eye_color = fct_infreq(eye_color)) You can see the new order of the factor levels with summary(): analytic_data_occasions %&gt;% select(where(is.factor)) %&gt;% summary() ## id sex eye_color ## 1 : 1 intersex: 2 hazel:100 ## 2 : 1 female :149 blue : 99 ## 3 : 1 male :147 brown: 98 ## 4 : 1 NA&#39;s : 2 NA&#39;s : 3 ## 5 : 1 ## 6 : 1 ## (Other):294 4.3.3 Numeric screening For numeric variables, you should search for impossible values. For example, in the context of this example you want to ensure none of the elapsed_times are impossible or so large they appear to be data entry errors. One options for doing so is the summary command again. This time, however, we use “is.numeric” in the where() command. However, when a survey has a large number of variables this type of output can become challenging to intrepret. analytic_data_occasions %&gt;% select(where(is.numeric)) %&gt;% summary() With larger numers of variables, it’s a good idea to use the skimr package. Additionally, it makes sense to look at sets of items rather than all of the variables at once. This is because sometime items and different response formats. For example, for one measure this responses may range from 1 to 5 whereas for another measure responses could range from 1 to 7. This is undesirable from a psychometric point of view, as discussed previously, but it if happens it makes sense to look at items in sets. Start by examining the range of non-scale items. In this case it’s only age. Examine the output to see if any of the age values are unreasonble. In the output p0 and p100 indicate the 0th percentile and the 100th percentile; that is the min an max values for the variable. Check to make sure none of the age values are unreasonably low or high. If they are, you may need to check the original data source or replace them with missing values. library(skimr) analytic_data_occasions %&gt;% select(age) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 age 3 20.52 2.05 17 20 24 Look the items in the first scale, self-esteem. Possible items responses for this scale range from 1 to 5, make sure they are all in this range. In the output p0 and p100 indicate the 0th percentile and the 100th percentile; that is the minimum an maximum values for the variable in these data. Examine the output to see if any of the esteem values are unreasonble (i.e., fall outside the 1 to 5 range). If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. analytic_data_occasions %&gt;% select(starts_with(&quot;t1_esteem&quot;)) %&gt;% skim() ## skim_variable n_missing mean sd p0 p50 p100 ## 1 t1_esteem1 24 3.39 0.54 3 3 5 ## 2 t1_esteem2 28 2.35 0.48 2 2 3 ## 3 t1_esteem3 31 3.96 0.37 3 4 5 ## 4 t1_esteem4 15 3.54 0.50 3 4 4 ## 5 t1_esteem5_rev15 35 2.22 0.47 1 2 3 Follow the same process t2_esteem items, t1_jobsat items, and t2_jobsat items. Possible items responses for these scale ranges from 1 to 5, make sure all responses are all in this range. If any values fall outside this range, you may need to check the original data source or replace them with missing values - as described previously. 4.3.4 Scale scores 4.3.4.1 Reverse key items Scale scores involve averaging, for each person, their score over several items to create an overall scale score. The first step in the creation of scales if correcting the values of any reverse-keyed items, as described previously. The logic of the math behind this correction is described previously in Surveys: Single occasion. In these data, the items use a 1 to 5 response scale so we use the reverse item correction for response scales that beging with 1. See Surveys: Single occasion for how to handle a response format that begins with zero - the math is different. The code below: * selects each column that ends with \"_rev15\" (i.e., both esteem and jobsat scales) * subtracts each value in that column from 6 * renames the column by removing \"_rev15\" because the reverse coding is complete. # Reverse code items analytic_data_occasions &lt;- analytic_data_occasions %&gt;% mutate(6 - across(.cols = ends_with(&quot;_rev15&quot;)) ) %&gt;% rename_with(.fn = str_remove, .cols = ends_with(&quot;_rev15&quot;), pattern = &quot;_rev15&quot;) 4.3.4.2 Creating scores The process we use for creating scale scores deletes item level data from analytic_data_survey. This is a desirable aspect of the process we it removes information we are no longer interested in from our analytic data. That said, before we create scale score, we create a backup on the item level data called analytic_data_items. We will need to use this backup later to compute the reliability of the scales we are creating. analytic_occasions_items &lt;- analytic_data_occasions We want to make a self_esteem scale and plan to select items using starts_with(“esteem”). But prior to doing this we make sure the start_with() command only gives us the items we want - and not additional unwanted items. The output below confirms there are not problems associated with using starts_with(“esteem”). analytic_data_occasions %&gt;% select(starts_with(&quot;t1_esteem&quot;)) %&gt;% glimpse() ## Rows: 300 ## Columns: 5 ## $ t1_esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, 3, 3, … ## $ t1_esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2, 2, 3… ## $ t1_esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA, NA, 3… ## $ t1_esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, 3, 3, … ## $ t1_esteem5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3, 4, 4, 3, 3, NA, 3, 3,… Repeat this set of commands using t2_esteem, t1_jobsat, and t2_jobsat. Make sure that those terms select only the relevant items and not others. analytic_data_occasions &lt;- analytic_data_occasions %&gt;% rowwise() %&gt;% mutate(esteem_t1 = mean(c_across(starts_with(&quot;t1_esteem&quot;)), na.rm = TRUE)) %&gt;% mutate(esteem_t2 = mean(c_across(starts_with(&quot;t2_esteem&quot;)), na.rm = TRUE)) %&gt;% mutate(jobsat_t1 = mean(c_across(starts_with(&quot;t1_jobsat&quot;)), na.rm = TRUE)) %&gt;% mutate(jobsat_t2 = mean(c_across(starts_with(&quot;t2_jobsat&quot;)), na.rm = TRUE)) %&gt;% ungroup() %&gt;% select(-starts_with(&quot;t1_esteem&quot;)) %&gt;% select(-starts_with(&quot;t2_esteem&quot;)) %&gt;% select(-starts_with(&quot;t1_jobsat&quot;)) %&gt;% select(-starts_with(&quot;t2_jobsat&quot;)) glimpse(analytic_data_occasions) ## Rows: 300 ## Columns: 8 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24, 17, N… ## $ sex &lt;fct&gt; male, female, male, female, male, female, male, female, m… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, blue, brown, hazel, … ## $ esteem_t1 &lt;dbl&gt; 3.200, 3.800, 3.800, 3.000, 3.400, 3.500, 3.000, 3.800, 3… ## $ esteem_t2 &lt;dbl&gt; 3.8, 4.4, 4.4, 3.6, 4.0, 4.2, 3.6, 4.4, 4.2, 4.2, 3.4, 4.… ## $ jobsat_t1 &lt;dbl&gt; 4.000, 5.000, 4.400, 3.500, 4.000, 3.800, 3.600, 4.600, 3… ## $ jobsat_t2 &lt;dbl&gt; 4.20, 4.40, 5.00, 4.20, 4.25, 4.40, 4.20, 5.20, 4.00, 4.5… 4.3.5 Pivot to tidy data But now comes the complicated point wher eyou need to pivot the data to longer. print(analytic_data_occasions) ## # A tibble: 300 x 8 ## id age sex eye_color esteem_t1 esteem_t2 jobsat_t1 jobsat_t2 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 23 male blue 3.2 3.8 4 4.2 ## 2 2 22 female brown 3.8 4.4 5 4.4 ## 3 3 18 male hazel 3.8 4.4 4.4 5 ## 4 4 23 female blue 3 3.6 3.5 4.2 ## 5 5 22 male &lt;NA&gt; 3.4 4 4 4.25 ## 6 6 17 female hazel 3.5 4.2 3.8 4.4 ## 7 7 23 male blue 3 3.6 3.6 4.2 ## 8 8 22 female brown 3.8 4.4 4.6 5.2 ## 9 9 17 male hazel 3.6 4.2 3.75 4 ## 10 10 NA female blue 3.6 4.2 3.8 4.5 ## # … with 290 more rows The pivot_longer() command below coverts our data to the tidy data format. In this command we specify the the columns with data by using esteem_t1:jobsat_t2, this selects these two columns and all of the columns between them. Each of these columns represents a dependent variable at a particular time in the format “variable_time” (e.g., esteem_t1). The code names_to = c(“.value”, “time”) explains this format to R. It indicates first part of the column name contains the name of the variable (expressed in the code as “.value”). It also indicates that the second part of the column name represents time. The line names_sep = \"_\" tells the R that the underscore character is used to separate the first part of the name from the second part of the name. When this code is excuted it creates a tidy version of data set stored in analytic_survey_tidy. analytic_occasion_tidy &lt;- analytic_data_occasions %&gt;% pivot_longer(esteem_t1:jobsat_t2, names_to = c(&quot;.value&quot;, &quot;time&quot;), names_sep = &quot;_&quot;) You can see the new data with the print() command. Notice that each participant has multiple rows associated with them. print(analytic_occasion_tidy) ## # A tibble: 600 x 7 ## id age sex eye_color time esteem jobsat ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 23 male blue t1 3.2 4 ## 2 1 23 male blue t2 3.8 4.2 ## 3 2 22 female brown t1 3.8 5 ## 4 2 22 female brown t2 4.4 4.4 ## 5 3 18 male hazel t1 3.8 4.4 ## 6 3 18 male hazel t2 4.4 5 ## 7 4 23 female blue t1 3 3.5 ## 8 4 23 female blue t2 3.6 4.2 ## 9 5 22 male &lt;NA&gt; t1 3.4 4 ## 10 5 22 male &lt;NA&gt; t2 4 4.25 ## # … with 590 more rows You now have three data sets The data analytic_occasion_tidy is appropriate for conducting a repeated measures ANOVA. The data analytic_data_occasions is appropriate for calculating descriptive statistics and correlations. The data analytic_occasions_items is appropriate for calculating the reliability of the scales you constructed. These data are ready for analysis. 4.4 Basic descriptive statistics 4.4.1 skim() One approach is the skim() command from the skimr package. This quickly provides the basic descriptive statistics. In the output for this command there are also several columns that beging with p: p0, p25, p50, p75, and p100 (p25 and p75 omitted in output due to space). These columns correspond to the 0th, 25th, 50th, 75th, and 100th percentiles, respectively. The median is the 50th percentile. The interquartile range is the range between p25 and p75. Notice that we run this command on the “wide” version of the data (analytic_data_occasions) rather than tidy version of the data (analytic_occasion_tidy). library(skimr) skim(analytic_data_occasions) ## skim_variable n_missing mean sd p0 p50 p100 ## 1 age 3 20.52 2.05 17.0 20.0 24.00 ## 2 esteem_t1 0 3.40 0.32 2.5 3.4 4.25 ## 3 esteem_t2 0 3.93 0.34 3.2 4.0 4.80 ## 4 jobsat_t1 0 3.91 0.43 2.0 4.0 5.00 ## 5 jobsat_t2 0 4.37 0.42 3.0 4.4 5.25 4.4.2 apa.cor.table() Another approach is the apa.cor.table() command from the apaTables package. This quickly provides the basic descriptive statistics as well as correlations among variable. As well, it will even create a Word document with this information, see Figure 4.2. Notice that we run this command on the “wide” version of the data (analytic_data_occasions) rather than tidy version of the data (analytic_occasion_tidy). library(apaTables) analytic_data_survey %&gt;% select(where(is.numeric)) %&gt;% apa.cor.table(filename = &quot;apa_descriptives.doc&quot;) FIGURE 4.2: Word document created by apa.cor.table 4.4.3 tidyverse A final approach uses tidyverse commands. This approach is oddly long - and we won’t describe how it works in detail. But, based on the information in the previous chapter you should be able to work out how this code works. Even though this code is long - it provide the ultimate in flexibility. If a new statistics is development that you want to use, you can simply include the command for it in the desired_descriptives list and it will be included in your table. Notice that we run this command on the “wide” version of the data (analytic_data_occasions) rather than tidy version of the data (analytic_occasion_tidy). library(tidyverse) # HMisc package must be installed. # Library command not needed for HMisc package. desired_descriptives &lt;- list( mean = ~mean(.x, na.rm = TRUE), CI95_LL = ~Hmisc::smean.cl.normal(.x)[2], CI95_UL = ~Hmisc::smean.cl.normal(.x)[3], sd = ~sd(.x, na.rm = TRUE), min = ~min(.x, na.rm = TRUE), max = ~max(.x, na.rm = TRUE), n = ~sum(!is.na(.x)) ) row_sum &lt;- analytic_data_occasions %&gt;% summarise(across(.cols = where(is.numeric), .fns = desired_descriptives, .names = &quot;{col}___{fn}&quot;)) long_summary &lt;- row_sum %&gt;% pivot_longer(cols = everything(), names_to = c(&quot;var&quot;, &quot;stat&quot;), names_sep = c(&quot;___&quot;), values_to = &quot;value&quot;) summary_table &lt;- long_summary %&gt;% pivot_wider(names_from = stat, values_from = value) summary_table_rounded &lt;- summary_table %&gt;% mutate(across(.cols = where(is.numeric), .fns= round, digits = 3)) %&gt;% as.data.frame() print(summary_table_rounded) ## var mean CI95_LL CI95_UL sd min max n ## 1 age 20.522 20.288 20.756 2.048 17.0 24.00 297 ## 2 esteem_t1 3.403 3.366 3.440 0.324 2.5 4.25 300 ## 3 esteem_t2 3.927 3.889 3.966 0.337 3.2 4.80 300 ## 4 jobsat_t1 3.905 3.856 3.955 0.435 2.0 5.00 300 ## 5 jobsat_t2 4.368 4.320 4.416 0.425 3.0 5.25 300 4.4.4 Cronbach’s alpha If you want Cronbach’s alpha to estimate the reliability of the scale, you can use the alpha command from the psych package with the code below. Note we have to use the item level data we created a copy of, called analytic_data_items. The glimpse() command illustrates this file has all the original items (after reverse-coding has been fixed). analytic_data_items %&gt;% glimpse() ## Rows: 300 ## Columns: 14 ## $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ age &lt;dbl&gt; 23, 22, 18, 23, 22, 17, 23, 22, 17, NA, 20, 17, 24, 17, N… ## $ sex &lt;fct&gt; male, female, male, female, male, female, male, female, m… ## $ eye_color &lt;fct&gt; blue, brown, hazel, blue, NA, hazel, blue, brown, hazel, … ## $ esteem1 &lt;dbl&gt; 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 3, 4, NA, NA, 3, 3, 3, 3, 3… ## $ esteem2 &lt;dbl&gt; 2, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, NA, 3, 2, 2, 2, 2, 3,… ## $ esteem3 &lt;dbl&gt; 4, 4, 4, 3, 4, 4, NA, 4, 4, 3, 4, 4, 4, NA, 4, NA, NA, 3,… ## $ esteem4 &lt;dbl&gt; 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, NA, 4, 3, 3, 4, NA, 3, 3, 4… ## $ esteem5 &lt;dbl&gt; 4, 4, 4, 4, 4, NA, NA, 4, 4, 4, 3, 4, 4, 3, 3, NA, 3, 3, … ## $ jobsat1 &lt;dbl&gt; 3, 5, 4, 3, 3, 3, 3, 5, 3, 3, 3, 4, 4, 3, 3, 4, 3, 3, 3, … ## $ jobsat2 &lt;dbl&gt; 5, 5, 5, NA, 5, 5, 4, 5, 4, 4, 3, 5, 3, 4, 5, 5, 4, 3, 5,… ## $ jobsat3 &lt;dbl&gt; 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 3,… ## $ jobsat4 &lt;dbl&gt; NA, 5, 5, 4, 4, 4, 4, 5, NA, 4, NA, 5, 4, 4, 4, 5, 4, 3, … ## $ jobsat5 &lt;dbl&gt; 5, NA, 5, 4, 5, 4, 4, 5, 5, 5, 4, NA, 4, 5, 4, 4, 4, 4, 5… We calculated reliability using psych::alpha() command. Cronbach’s alpha is labeled “raw alpha” in the output. Cronbach’s alpha is an estimate of the proportion of varibilty in observed scores that is due to actual differences among participants (rather than measurement error). rxx_alpha &lt;- analytic_occasions_items %&gt;% select(starts_with(&quot;t1_esteem&quot;)) %&gt;% psych::alpha() print(rxx_alpha$total) ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.6622 0.6634 0.6173 0.2827 1.97 0.03035 3.403 0.3239 0.2927 "]
]
